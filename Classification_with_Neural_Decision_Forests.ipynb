{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790a99d5-b5ac-417e-8b90-0cb315ab8463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 20:58:47.933761: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762459127.944884   30316 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762459127.948306   30316 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762459127.958107   30316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762459127.958115   30316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762459127.958117   30316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762459127.958118   30316 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-06 20:58:47.961033: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import StringLookup\n",
    "from keras import ops\n",
    "\n",
    "\n",
    "from tensorflow import data as tf_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3644a1f-8209-45b0-b2cb-138f85e94c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (32561, 15)\n",
      "Test dataset shape: (16282, 15)\n"
     ]
    }
   ],
   "source": [
    "CSV_HEADER = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]\n",
    "\n",
    "train_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    ")\n",
    "train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "test_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    ")\n",
    "test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n",
    "\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "print(f\"Test dataset shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1950a5-1cb6-45f3-b9e1-b275d94e06e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[1:]\n",
    "test_data.income_bracket = test_data.income_bracket.apply(\n",
    "    lambda value: value.replace(\".\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a75f7e68-996d-4665-99da-120ebe8fffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=False)\n",
    "test_data.to_csv(test_data_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d5d17b4-9c47-46a6-9e86-d9c77f93f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"age\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n",
    "    \"education\": sorted(list(train_data[\"education\"].unique())),\n",
    "    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n",
    "    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n",
    "    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n",
    "    \"race\": sorted(list(train_data[\"race\"].unique())),\n",
    "    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n",
    "    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n",
    "}\n",
    "# A list of the columns to ignore from the dataset.\n",
    "IGNORE_COLUMN_NAMES = [\"fnlwgt\"]\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + IGNORE_COLUMN_NAMES else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"income_bracket\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\" <=50K\", \" >50K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7568d0b7-1a87-4d84-a58c-f329b57a1d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762459182.318612   30316 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 680 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "target_label_lookup = StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "lookup_dict = {}\n",
    "for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "    vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "    # Create a lookup to convert a string values to an integer indices.\n",
    "    # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "    # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "    lookup = StringLookup(vocabulary=vocabulary, mask_token=None, num_oov_indices=0)\n",
    "    lookup_dict[feature_name] = lookup\n",
    "\n",
    "\n",
    "def encode_categorical(batch_x, batch_y):\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        batch_x[feature_name] = lookup_dict[feature_name](batch_x[feature_name])\n",
    "\n",
    "    return batch_x, batch_y\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    dataset = (\n",
    "        tf_data.experimental.make_csv_dataset(\n",
    "            csv_file_path,\n",
    "            batch_size=batch_size,\n",
    "            column_names=CSV_HEADER,\n",
    "            column_defaults=COLUMN_DEFAULTS,\n",
    "            label_name=TARGET_FEATURE_NAME,\n",
    "            num_epochs=1,\n",
    "            header=False,\n",
    "            na_value=\"?\",\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "        .map(lambda features, target: (features, target_label_lookup(target)))\n",
    "        .map(encode_categorical)\n",
    "    )\n",
    "\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62d0e1a-7124-4cd3-80f9-8e024685e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"int32\"\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2127060c-94ad-43dc-b5ae-6e4dbd5e10d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs):\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert a string values to an integer indices.\n",
    "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "            value_index = inputs[feature_name]\n",
    "            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n",
    "            )\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_feature = embedding(value_index)\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            encoded_feature = inputs[feature_name]\n",
    "            if inputs[feature_name].shape[-1] is None:\n",
    "                encoded_feature = keras.ops.expand_dims(encoded_feature, -1)\n",
    "\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    encoded_features = layers.concatenate(encoded_features)\n",
    "    return encoded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0145f93e-f33d-4998-ac65-c45825b2675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDecisionTree(keras.Model):\n",
    "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.num_leaves = 2**depth\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Create a mask for the randomly selected features.\n",
    "        num_used_features = int(num_features * used_features_rate)\n",
    "        one_hot = np.eye(num_features)\n",
    "        sampled_feature_indices = np.random.choice(\n",
    "            np.arange(num_features), num_used_features, replace=False\n",
    "        )\n",
    "        self.used_features_mask = ops.convert_to_tensor(\n",
    "            one_hot[sampled_feature_indices], dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Initialize the weights of the classes in leaves.\n",
    "        self.pi = self.add_weight(\n",
    "            initializer=\"random_normal\",\n",
    "            shape=[self.num_leaves, self.num_classes],\n",
    "            dtype=\"float32\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Initialize the stochastic routing layer.\n",
    "        self.decision_fn = layers.Dense(\n",
    "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        batch_size = ops.shape(features)[0]\n",
    "\n",
    "        # Apply the feature mask to the input features.\n",
    "        features = ops.matmul(\n",
    "            features, ops.transpose(self.used_features_mask)\n",
    "        )  # [batch_size, num_used_features]\n",
    "        # Compute the routing probabilities.\n",
    "        decisions = ops.expand_dims(\n",
    "            self.decision_fn(features), axis=2\n",
    "        )  # [batch_size, num_leaves, 1]\n",
    "        # Concatenate the routing probabilities with their complements.\n",
    "        decisions = layers.concatenate(\n",
    "            [decisions, 1 - decisions], axis=2\n",
    "        )  # [batch_size, num_leaves, 2]\n",
    "\n",
    "        mu = ops.ones([batch_size, 1, 1])\n",
    "\n",
    "        begin_idx = 1\n",
    "        end_idx = 2\n",
    "        # Traverse the tree in breadth-first order.\n",
    "        for level in range(self.depth):\n",
    "            mu = ops.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
    "            mu = ops.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
    "            level_decisions = decisions[\n",
    "                :, begin_idx:end_idx, :\n",
    "            ]  # [batch_size, 2 ** level, 2]\n",
    "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
    "            begin_idx = end_idx\n",
    "            end_idx = begin_idx + 2 ** (level + 1)\n",
    "\n",
    "        mu = ops.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
    "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
    "        outputs = ops.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28bf9a09-4393-40b1-95fe-2444f9c5ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDecisionForest(keras.Model):\n",
    "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
    "        super().__init__()\n",
    "        self.ensemble = []\n",
    "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
    "        # Each tree will have its own randomly selected input features to use.\n",
    "        for _ in range(num_trees):\n",
    "            self.ensemble.append(\n",
    "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
    "            )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
    "        batch_size = ops.shape(inputs)[0]\n",
    "        outputs = ops.zeros([batch_size, num_classes])\n",
    "\n",
    "        # Aggregate the outputs of trees in the ensemble.\n",
    "        for tree in self.ensemble:\n",
    "            outputs += tree(inputs)\n",
    "        # Divide the outputs by the ensemble size to get the average.\n",
    "        outputs /= len(self.ensemble)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "513d0294-eee9-4f5e-9068-f5e799d317d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 265\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    train_dataset = get_dataset_from_csv(\n",
    "        train_data_file, shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    model.fit(train_dataset, epochs=num_epochs)\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    print(\"Evaluating the model on the test data...\")\n",
    "    test_dataset = get_dataset_from_csv(test_data_file, batch_size=batch_size)\n",
    "\n",
    "    _, accuracy = model.evaluate(test_dataset)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c0a7925-2ff0-4472-b2da-0ca69170ac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xy/Desktop/ml/tf_2.20/tf_2.20/lib/python3.10/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: {'age': 'age', 'education_num': 'education_num', 'capital_gain': 'capital_gain', 'capital_loss': 'capital_loss', 'hours_per_week': 'hours_per_week', 'workclass': 'workclass', 'education': 'education', 'marital_status': 'marital_status', 'occupation': 'occupation', 'relationship': 'relationship', 'race': 'race', 'gender': 'gender', 'native_country': 'native_country'}\n",
      "Received: inputs=OrderedDict([('age', 'Tensor(shape=(None,))'), ('workclass', 'Tensor(shape=(None,))'), ('fnlwgt', 'Tensor(shape=(None,))'), ('education', 'Tensor(shape=(None,))'), ('education_num', 'Tensor(shape=(None,))'), ('marital_status', 'Tensor(shape=(None,))'), ('occupation', 'Tensor(shape=(None,))'), ('relationship', 'Tensor(shape=(None,))'), ('race', 'Tensor(shape=(None,))'), ('gender', 'Tensor(shape=(None,))'), ('capital_gain', 'Tensor(shape=(None,))'), ('capital_loss', 'Tensor(shape=(None,))'), ('hours_per_week', 'Tensor(shape=(None,))'), ('native_country', 'Tensor(shape=(None,))')])\n",
      "  warnings.warn(msg)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762459245.135742   30530 service.cc:152] XLA service 0x7d080c003040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1762459245.135760   30530 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2025-11-06 21:00:45.201614: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1762459245.387286   30530 cuda_dnn.cc:529] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    104/Unknown \u001b[1m3s\u001b[0m 1ms/step - loss: 0.5464 - sparse_categorical_accuracy: 0.8106"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762459246.468920   30530 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.4485 - sparse_categorical_accuracy: 0.8292\n",
      "Epoch 2/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3567 - sparse_categorical_accuracy: 0.8385\n",
      "Epoch 3/10\n",
      "\u001b[1m  1/123\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3309 - sparse_categorical_accuracy: 0.8453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 21:00:47.832268: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-11-06 21:00:47.832283: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_28]]\n",
      "2025-11-06 21:00:47.832294: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6061483516878966009\n",
      "2025-11-06 21:00:47.832297: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 2363787253057566181\n",
      "2025-11-06 21:00:47.832300: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 728317354515981321\n",
      "/home/xy/Desktop/ml/tf_2.20/tf_2.20/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:164: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-11-06 21:00:48.007975: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n",
      "2025-11-06 21:00:48.007989: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6061483516878966009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3455 - sparse_categorical_accuracy: 0.8416\n",
      "Epoch 4/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3394 - sparse_categorical_accuracy: 0.8426\n",
      "Epoch 5/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3351 - sparse_categorical_accuracy: 0.8439\n",
      "Epoch 6/10\n",
      "\u001b[1m  1/123\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3217 - sparse_categorical_accuracy: 0.8491"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 21:00:48.361190: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_16]]\n",
      "2025-11-06 21:00:48.361203: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6061483516878966009\n",
      "2025-11-06 21:00:48.361208: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 2363787253057566181\n",
      "2025-11-06 21:00:48.361211: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 728317354515981321\n",
      "2025-11-06 21:00:48.361217: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 12706924109028515038\n",
      "2025-11-06 21:00:48.361225: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17966567053742137186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3320 - sparse_categorical_accuracy: 0.8453\n",
      "Epoch 7/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3285 - sparse_categorical_accuracy: 0.8461\n",
      "Epoch 8/10\n",
      "\u001b[1m  1/123\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3054 - sparse_categorical_accuracy: 0.8566"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 21:00:48.699477: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6061483516878966009\n",
      "2025-11-06 21:00:48.865614: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6061483516878966009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3249 - sparse_categorical_accuracy: 0.8495\n",
      "Epoch 9/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3214 - sparse_categorical_accuracy: 0.8515\n",
      "Epoch 10/10\n",
      "\u001b[1m  1/123\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2862 - sparse_categorical_accuracy: 0.8642"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 21:00:49.035423: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2025-11-06 21:00:49.207864: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 13248148859195335131\n",
      "2025-11-06 21:00:49.207875: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6061483516878966009\n",
      "2025-11-06 21:00:49.207877: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 14834963281502331554\n",
      "2025-11-06 21:00:49.207888: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 12706924109028515038\n",
      "2025-11-06 21:00:49.207894: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17966567053742137186\n",
      "2025-11-06 21:00:49.207900: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 2363787253057566181\n",
      "2025-11-06 21:00:49.207903: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9734464469072307611\n",
      "2025-11-06 21:00:49.207905: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 728317354515981321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3171 - sparse_categorical_accuracy: 0.8550\n",
      "Model training finished\n",
      "Evaluating the model on the test data...\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3426 - sparse_categorical_accuracy: 0.8389\n",
      "Test accuracy: 83.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 21:00:50.520281: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 2363787253057566181\n",
      "2025-11-06 21:00:50.520294: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 937716259546149190\n",
      "2025-11-06 21:00:50.520298: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10690450492091732902\n"
     ]
    }
   ],
   "source": [
    "num_trees = 10\n",
    "depth = 10\n",
    "used_features_rate = 1.0\n",
    "num_classes = len(TARGET_LABELS)\n",
    "\n",
    "\n",
    "def create_tree_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "    features = layers.BatchNormalization()(features)\n",
    "    num_features = features.shape[1]\n",
    "\n",
    "    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
    "\n",
    "    outputs = tree(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tree_model = create_tree_model()\n",
    "run_experiment(tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "006a90b6-1a6d-414b-b3f8-2253f57b81ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 39ms/step - loss: 0.4596 - sparse_categorical_accuracy: 0.8165\n",
      "Epoch 2/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3540 - sparse_categorical_accuracy: 0.8399\n",
      "Epoch 3/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3412 - sparse_categorical_accuracy: 0.8418\n",
      "Epoch 4/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3354 - sparse_categorical_accuracy: 0.8435\n",
      "Epoch 5/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3315 - sparse_categorical_accuracy: 0.8450\n",
      "Epoch 6/10\n",
      "\u001b[1m 42/123\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3357 - sparse_categorical_accuracy: 0.8399"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 21:01:24.758177: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_4]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3283 - sparse_categorical_accuracy: 0.8460\n",
      "Epoch 7/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3254 - sparse_categorical_accuracy: 0.8472\n",
      "Epoch 8/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3227 - sparse_categorical_accuracy: 0.8487\n",
      "Epoch 9/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3202 - sparse_categorical_accuracy: 0.8496\n",
      "Epoch 10/10\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3178 - sparse_categorical_accuracy: 0.8503\n",
      "Model training finished\n",
      "Evaluating the model on the test data...\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 0.3326 - sparse_categorical_accuracy: 0.8453\n",
      "Test accuracy: 84.53%\n"
     ]
    }
   ],
   "source": [
    "num_trees = 25\n",
    "depth = 5\n",
    "used_features_rate = 0.5\n",
    "\n",
    "\n",
    "def create_forest_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "    features = layers.BatchNormalization()(features)\n",
    "    num_features = features.shape[1]\n",
    "\n",
    "    forest_model = NeuralDecisionForest(\n",
    "        num_trees, depth, num_features, used_features_rate, num_classes\n",
    "    )\n",
    "\n",
    "    outputs = forest_model(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "forest_model = create_forest_model()\n",
    "\n",
    "run_experiment(forest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca8c69c-a520-4a5a-bd58-f1159b4b9116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7931563-7c73-44e6-aa29-778fb6b11bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b001f65d-b042-4f2c-a882-20349d7881b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83107c5-bfba-4638-ab23-38535bdec190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6bffa-a875-4693-88f4-f4d07cc3fb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0058a-9c31-46e6-9f60-58d76322ac93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887dabf5-764d-41f8-b66c-43ef70574502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93a4e9-1e8c-413f-88d6-59c59dd9fb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
